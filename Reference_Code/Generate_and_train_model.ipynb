{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e860da-0947-4309-8f40-a6e7cef1da35",
   "metadata": {},
   "source": [
    "## Dataset Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d697973-2385-4d5d-b550-210a0c193897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Variables\n",
    "from torchsig.signals.signal_lists import TorchSigSignalLists\n",
    "from torchsig.transforms.transforms import ComplexTo2D\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10a4006-735a-4ca3-86d1-e8fe91615202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "root = \"./datasets/PSK_v_AM_v_FM/classifier_example\"\n",
    "os.makedirs(root, exist_ok=True)\n",
    "os.makedirs(root + \"/train\", exist_ok=True)\n",
    "os.makedirs(root + \"/val\", exist_ok=True)\n",
    "os.makedirs(root + \"/test\", exist_ok=True)\n",
    "fft_size = 1024\n",
    "num_iq_samples = fft_size ** 2\n",
    "num_iq_samples_dataset = num_iq_samples\n",
    "num_samples_per_class = 1000 # the number of samples for each modulation in the class list to create\n",
    "percent_validation = 0.2 # the number of validation data points to generate as a percentage of the num of train data points\n",
    "class_list = TorchSigSignalLists.all_signals\n",
    "family_list = TorchSigSignalLists.family_list\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7e812f-6b5c-4add-b8e9-9326bc930168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'ask', 'chirp', 'fm', 'fsk', 'ofdm', 'ook', 'psk', 'qam', 'tone']\n",
      "['psk', 'am', 'fm', 'ofdm']\n",
      "['ook', '4ask', '8ask', '16ask', '32ask', '64ask', '2fsk', '2gfsk', '2msk', '2gmsk', '4fsk', '4gfsk', '4msk', '4gmsk', '8fsk', '8gfsk', '8msk', '8gmsk', '16fsk', '16gfsk', '16msk', '16gmsk', 'bpsk', 'qpsk', '8psk', '16psk', '32psk', '64psk', '16qam', '32qam', '32qam_cross', '64qam', '128qam_cross', '256qam', '512qam_cross', '1024qam', 'ofdm-64', 'ofdm-72', 'ofdm-128', 'ofdm-180', 'ofdm-256', 'ofdm-300', 'ofdm-512', 'ofdm-600', 'ofdm-900', 'ofdm-1024', 'ofdm-1200', 'ofdm-2048', 'fm', 'am-dsb-sc', 'am-dsb', 'am-lsb', 'am-usb', 'lfm_data', 'lfm_radar', 'chirpss', 'tone']\n",
      "['bpsk', 'fm', 'am-dsb-sc', 'ofdm-128', 'lfm_radar', 'chirpss', 'ook', '16ask', '64qam']\n"
     ]
    }
   ],
   "source": [
    "print(family_list)\n",
    "family_list = ['psk', 'am', 'fm', 'ofdm']\n",
    "print(family_list)\n",
    "print(class_list)\n",
    "class_list = ['bpsk','fm' , 'am-dsb-sc', 'ofdm-128', 'lfm_radar', 'chirpss', 'ook', '16ask', '64qam']\n",
    "print(class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6109ffc8-66e7-4728-bef3-38fdf5d5011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(class_list)\n",
    "num_samples_train = len(class_list) * num_samples_per_class # \n",
    "num_samples_val = int(len(class_list) * num_samples_per_class * percent_validation)\n",
    "impairment_level = 0\n",
    "seed = 123456789\n",
    " # IQ-based mod-rec only operates on 1 signal\n",
    "num_signals_max = 1\n",
    "num_signals_min = 1\n",
    "\n",
    "# ComplexTo2D turns a IQ array of complex values into a 2D array, with one channel for the real component, while the other is for the imaginary component\n",
    "transforms = [ComplexTo2D()]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d7301-d923-4ac1-953a-8884267a6ac8",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b05804c-479d-4301-97c2-5f86262b8ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914fdce7e1164a069c4486f28ff469ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted folder: datasets/PSK_v_AM_v_FM/classifier_example/train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd27715f260c4443a187356f19fe0102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted folder: datasets/PSK_v_AM_v_FM/classifier_example/val\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from torchsig.datasets.dataset_metadata import DatasetMetadata\n",
    "from torchsig.datasets.datasets import TorchSigIterableDataset, StaticTorchSigDataset\n",
    "from torchsig.utils.data_loading import WorkerSeedingDataLoader\n",
    "from torchsig.utils.writer import DatasetCreator\n",
    "\n",
    "dataset_metadata = DatasetMetadata(\n",
    "    num_iq_samples_dataset = num_iq_samples_dataset,\n",
    "    fft_size = fft_size,\n",
    "    class_list = class_list,\n",
    "    num_signals_max = num_signals_max,\n",
    "    num_signals_min = num_signals_min,\n",
    ")\n",
    "\n",
    "train_dataset = TorchSigIterableDataset(dataset_metadata, transforms=transforms, target_labels=None)\n",
    "val_dataset = TorchSigIterableDataset(dataset_metadata, transforms=transforms, target_labels=None)\n",
    "\n",
    "train_dataloader = WorkerSeedingDataLoader(train_dataset, batch_size=1, collate_fn = lambda x: x)\n",
    "val_dataloader = WorkerSeedingDataLoader(val_dataset, collate_fn = lambda x: x)\n",
    "\n",
    "#print(f\"Data shape: {data.shape}\")\n",
    "#print(f\"Targets: {targets}\")\n",
    "# next(train_dataset)\n",
    "\n",
    "dc = DatasetCreator(\n",
    "    dataloader=train_dataloader,\n",
    "    root = f\"{root}/train\",\n",
    "    overwrite=True,\n",
    "    dataset_length=num_samples_train\n",
    ")\n",
    "dc.create()\n",
    "\n",
    "\n",
    "dc = DatasetCreator(\n",
    "    dataloader=val_dataloader,\n",
    "    root = f\"{root}/val\",\n",
    "    overwrite=True,\n",
    "    dataset_length=num_samples_val\n",
    ")\n",
    "dc.create()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b9cf09-c86a-4277-b720-6186c3524597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.05417608,  0.07383577,  0.09509687, ..., -0.03767033,\n",
      "        -0.12575406, -0.00108993],\n",
      "       [-0.02886894,  0.0069131 , -0.08293553, ...,  0.0399799 ,\n",
      "        -0.10247673,  0.0880217 ]], dtype=float32), 0)\n",
      "90000\n",
      "18000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "train_dataset = StaticTorchSigDataset(\n",
    "    root = f\"{root}/train\",\n",
    "    target_labels=[\"class_index\"]\n",
    ")\n",
    "val_dataset = StaticTorchSigDataset(\n",
    "    root = f\"{root}/val\",\n",
    "    target_labels=[\"class_index\"]\n",
    ")\n",
    "\n",
    "train_dataloader = WorkerSeedingDataLoader(train_dataset, batch_size=1, num_workers=127)\n",
    "val_dataloader = WorkerSeedingDataLoader(val_dataset, num_workers=127)\n",
    "\n",
    "print(train_dataset[0])\n",
    "#print(\"done\")\n",
    "print(len(train_dataloader))\n",
    "print(len(val_dataloader))\n",
    "next(iter(train_dataloader))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a38bdca-f6f2-4fd1-a2a8-6afdffd24278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_data = []\\ntrain_label = []\\nfor x in train_dataset:\\n    #hold = (x[0],x[1])\\n    train_data.append(x[0])\\n    train_label.append(x[1])\\n    \\nval_data = []\\nval_label = []\\nfor x in val_dataset:\\n    #hold = (x[0],x[1])\\n    val_data.append(x[0])\\n    val_label.append(x[1])\\n\\nprint(\"done\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_data = []\n",
    "train_label = []\n",
    "for x in train_dataset:\n",
    "    #hold = (x[0],x[1])\n",
    "    train_data.append(x[0])\n",
    "    train_label.append(x[1])\n",
    "    \n",
    "val_data = []\n",
    "val_label = []\n",
    "for x in val_dataset:\n",
    "    #hold = (x[0],x[1])\n",
    "    val_data.append(x[0])\n",
    "    val_label.append(x[1])\n",
    "\n",
    "print(\"done\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1185e0ce-ff00-4e6c-a923-c617fc62d561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "##Trying nesting the arrays\n",
    "train_data = []\n",
    "train_label = []\n",
    "for x in train_dataset:\n",
    "    #hold = (x[0],x[1])\n",
    "    train_data.append([x[0]])\n",
    "    train_label.append(x[1])\n",
    "    \n",
    "val_data = []\n",
    "val_label = []\n",
    "for x in val_dataset:\n",
    "    #hold = (x[0],x[1])\n",
    "    val_data.append([x[0]])\n",
    "    val_label.append(x[1])\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8277d39a-ae24-4579-a59d-20b57a12a33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.05417608,  0.07383577,  0.09509687, ..., -0.03767033,\n",
      "        -0.12575406, -0.00108993],\n",
      "       [-0.02886894,  0.0069131 , -0.08293553, ...,  0.0399799 ,\n",
      "        -0.10247673,  0.0880217 ]], dtype=float32)]\n",
      "[[-0.05417608  0.07383577  0.09509687 ... -0.03767033 -0.12575406\n",
      "  -0.00108993]\n",
      " [-0.02886894  0.0069131  -0.08293553 ...  0.0399799  -0.10247673\n",
      "   0.0880217 ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7605c0b1-23b4-41e2-bccd-d139977d4272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"A simple context manager for timing code blocks.\"\"\"\n",
    "    def __init__(self, timings_dict, key):\n",
    "        self.timings = timings_dict\n",
    "        self.key = key\n",
    "        self.start_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"[{self.key}] starting...\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        self.timings[self.key] = elapsed\n",
    "        print(f\"[{self.key}] finished in {elapsed:.2f} seconds.\")\n",
    "\n",
    "def report_timings(timings):\n",
    "    \"\"\"Prints a summary of the timings.\"\"\"\n",
    "    print(\"\\n--- Timing Report ---\")\n",
    "    total_time = 0\n",
    "    for key, value in timings.items():\n",
    "        print(f\"  - {key}: {value:.2f} seconds\")\n",
    "        total_time += value\n",
    "    print(\"---------------------\")\n",
    "    print(f\"  Total Elapsed Time: {total_time:.2f} seconds\")\n",
    "\n",
    "def save_report(report_path, config, model_summary, history, final_accuracy, timings):\n",
    "    \"\"\"Saves a detailed report of the training session to a text file.\"\"\"\n",
    "    print(f\"\\n--- Saving Training Report to {report_path} ---\")\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(\"           TRAINING SESSION REPORT\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        # --- 1. Configuration ---\n",
    "        f.write(\"--- 1. Training Configuration ---\\n\")\n",
    "        for key, value in config.items():\n",
    "            f.write(f\"  - {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # --- 2. Model Architecture ---\n",
    "        f.write(\"--- 2. Model Architecture ---\\n\")\n",
    "        f.write(model_summary + \"\\n\\n\")\n",
    "\n",
    "        # --- 3. Training History ---\n",
    "        f.write(\"--- 3. Training History (Epoch-wise) ---\\n\")\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        header = f\"{'Epoch':<7} | {'Train Loss':<12} | {'Train Acc (%)':<15} | {'Val Acc (%)':<13}\\n\"\n",
    "        f.write(header)\n",
    "        f.write(\"-\" * len(header) + \"\\n\")\n",
    "        for i, epoch in enumerate(epochs):\n",
    "            loss = history['train_loss'][i]\n",
    "            train_acc = history['train_acc'][i]\n",
    "            val_acc = history['val_acc'][i]\n",
    "            f.write(f\"{epoch:<7} | {loss:<12.4f} | {train_acc:<15.2f} | {val_acc:<13.2f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # --- 4. Final Performance ---\n",
    "        f.write(\"--- 4. Final Performance on Test Set ---\\n\")\n",
    "        f.write(f\"  - Accuracy: {final_accuracy:.2f} %\\n\\n\")\n",
    "\n",
    "        # --- 5. Timing Report ---\n",
    "        f.write(\"--- 5. Timing Report ---\\n\")\n",
    "        total_time = 0\n",
    "        for key, value in timings.items():\n",
    "            f.write(f\"  - {key}: {value:.2f} seconds\\n\")\n",
    "            total_time += value\n",
    "        f.write(\"---------------------\\n\")\n",
    "        f.write(f\"  Total Elapsed Time: {total_time:.2f} seconds\\n\")\n",
    "    \n",
    "    print(\"Report saved successfully.\")\n",
    "\n",
    "class DynamicCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A dynamically configurable Convolutional Neural Network (CNN).\n",
    "    The user can specify the architecture including convolutional layers,\n",
    "    pooling layers, and fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes, conv_layers_config, fc_layers_config, num_iq_samples):\n",
    "        \"\"\"\n",
    "        Initializes the DynamicCNN model.\n",
    "\n",
    "        Args:\n",
    "            input_channels (int): Number of input channels (e.g., 3 for RGB images).\n",
    "            num_classes (int): Number of output classes.\n",
    "            conv_layers_config (list of tuples): Each tuple defines a convolutional block.\n",
    "                Format: (out_channels, kernel_size, stride, padding, use_pooling)\n",
    "                'use_pooling' is a boolean to add a MaxPool2d layer after the conv layer.\n",
    "            fc_layers_config (list of int): A list where each integer is the number of\n",
    "                neurons in a fully connected layer.\n",
    "        \"\"\"\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "\n",
    "        in_channels = input_channels\n",
    "\n",
    "        print(\"model intial in_channels: \", in_channels)\n",
    "\n",
    "        # Create convolutional layers dynamically\n",
    "        for out_channels, kernel_size, stride, padding, use_pooling in conv_layers_config:\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "            )\n",
    "            if use_pooling:\n",
    "                self.pool_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            else:\n",
    "                self.pool_layers.append(nn.Identity()) # Placeholder if no pooling\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # To determine the input size of the first fully connected layer,\n",
    "        # we need to do a forward pass with a dummy tensor.\n",
    "        # This is a common practice when the architecture is dynamic.\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, input_channels, num_iq_samples[0], num_iq_samples[1]) # !!! Changeing this to account for shape of IQ data\n",
    "            dummy_output = self._forward_conv(dummy_input)\n",
    "            flattened_size = dummy_output.view(dummy_output.size(0), -1).shape[1]\n",
    "\n",
    "        in_features = flattened_size\n",
    "\n",
    "        print(\"in features for first FC layer: \", in_features)\n",
    "\n",
    "        # Create fully connected layers dynamically\n",
    "        for out_features in fc_layers_config:\n",
    "            self.fc_layers.append(nn.Linear(in_features, out_features))\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        \"\"\"Helper function to forward through conv layers only.\"\"\"\n",
    "        count = 0\n",
    "        for conv, pool in zip(self.conv_layers, self.pool_layers):\n",
    "            '''\n",
    "            count+=1\n",
    "            print(\"layer: \", count, \" with input len\", len(x), \"and input: \", x)\n",
    "            print(\"x[0]: \", x[0]) \n",
    "            print(\"len x[0]: \", len(x[0]))\n",
    "            print(\"x[0][0]: \", x[0][0]) \n",
    "            print(\"len x[0][0]: \", len(x[0][0])) \n",
    "            '''\n",
    "            x = torch.relu(conv(x))\n",
    "            x = pool(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        print(\"start x: \", x) \n",
    "        print(\"len start x: \", len(x)) \n",
    "        print(\"start x[0]: \", x[0]) \n",
    "        print(\"len start x[0]: \", len(x[0]))\n",
    "        print(\"start x[0][0]: \", x[0][0]) \n",
    "        print(\"len start x[0][0]: \", len(x[0][0])) \n",
    "        '''\n",
    "        x = self._forward_conv(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Forward pass through fully connected layers\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = torch.relu(fc_layer(x))\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95d1d4d-829f-4d9e-adf9-5fdb366cc387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on a given dataset.\n",
    "    Used for validation and training accuracy checks.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Trains the PyTorch model and tracks history for plotting.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    total_training_time = 0\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train() # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluate on training and validation set after each epoch\n",
    "        train_acc = evaluate_model(model, train_loader, device)\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        total_training_time += epoch_duration\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}% -- Took {epoch_duration:.2f}s')\n",
    "            \n",
    "    print(f\"Finished training. Total training time: {total_training_time:.2f}s\")\n",
    "    return history, total_training_time\n",
    "\n",
    "def test_model(model, test_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Tests the PyTorch model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"Starting testing...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy:.2f} %')\n",
    "    return accuracy\n",
    "\n",
    "def plot_results(history):\n",
    "    \"\"\"\n",
    "    Plots training and validation metrics using matplotlib.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Plotting Results ---\")\n",
    "    \n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "    except ImportError:\n",
    "        print(\"matplotlib not found. Please install it ('pip install matplotlib') to plot results.\")\n",
    "        return\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    # Plot Training & Validation Accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['train_acc'], 'o-', label='Training Accuracy', color='b')\n",
    "    plt.plot(epochs, history['val_acc'], 'o-', label='Validation Accuracy', color='r')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Training Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['train_loss'], 'o-', label='Training Loss', color='g')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    print(\"Results plot saved as 'training_results.png'\")\n",
    "    # You can uncomment the line below to display the plot directly\n",
    "    # plt.show()\n",
    "\n",
    "def save_model(model, path=\"cnn_model.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the trained model's state dictionary.\n",
    "    Handles models wrapped in nn.DataParallel.\n",
    "    \"\"\"\n",
    "    print(f\"Saving model to {path}...\")\n",
    "    # If the model is wrapped in DataParallel, save the underlying model's state_dict\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        torch.save(model.module.state_dict(), path)\n",
    "    else:\n",
    "        torch.save(model.state_dict(), path)\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "def load_model(model_class, path, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Loads a model's state dictionary from a file.\n",
    "    Args:\n",
    "        model_class: The class of the model to be instantiated.\n",
    "        path (str): The path to the saved model file.\n",
    "        *args, **kwargs: Arguments needed to instantiate the model class.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from {path}...\")\n",
    "    model = model_class(*args, **kwargs)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(\"Model loaded.\")\n",
    "    return model\n",
    "\n",
    "def analyze_dataset(loader, dataset_name=\"Training\"):\n",
    "    \"\"\"\n",
    "    Performs and prints a statistical analysis on the dataset from a DataLoader.\n",
    "    Calculates class distribution, image dimensions, and pixel value statistics.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): The DataLoader for the dataset to analyze.\n",
    "        dataset_name (str): Name of the dataset (e.g., \"Training\", \"Testing\").\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing {dataset_name} Dataset ---\")\n",
    "\n",
    "    if not hasattr(loader, 'dataset'):\n",
    "        print(\"DataLoader does not have a 'dataset' attribute.\")\n",
    "        return\n",
    "\n",
    "    dataset = loader.dataset\n",
    "    \n",
    "    # Handle datasets wrapped in torch.utils.data.Subset (from random_split)\n",
    "    if isinstance(dataset, torch.utils.data.Subset):\n",
    "        # Get labels and data from the original dataset using subset indices\n",
    "        labels = [dataset.dataset.tensors[1][i] for i in dataset.indices]\n",
    "        data_tensor = torch.stack([dataset.dataset.tensors[0][i] for i in dataset.indices])\n",
    "        labels_tensor = torch.tensor(labels)\n",
    "    elif isinstance(dataset, TensorDataset):\n",
    "        # Handle regular TensorDataset\n",
    "        data_tensor = dataset.tensors[0]\n",
    "        labels_tensor = dataset.tensors[1]\n",
    "    else:\n",
    "        print(f\"Dataset type {type(dataset).__name__} is not supported for analysis.\")\n",
    "        return\n",
    "\n",
    "    if len(labels_tensor) == 0:\n",
    "        print(\"No labels found to analyze.\")\n",
    "        return\n",
    "\n",
    "    # 1. Class Distribution\n",
    "    print(\"Class Distribution:\")\n",
    "    unique_classes, counts = torch.unique(labels_tensor, return_counts=True)\n",
    "    class_dist = dict(zip(unique_classes.tolist(), counts.tolist()))\n",
    "    \n",
    "    for cls in sorted(class_dist.keys()):\n",
    "        percentage = (class_dist[cls] / len(labels_tensor)) * 100\n",
    "        print(f\"  Class {cls}: {class_dist[cls]} samples ({percentage:.2f}%)\")\n",
    "\n",
    "    # 2. General Metrics\n",
    "    num_samples = data_tensor.shape[0]\n",
    "    print(f\"\\nTotal Samples: {num_samples}\")\n",
    "\n",
    "    if data_tensor.dim() == 4:  # Expected format: (N, C, H, W)\n",
    "        num_channels = data_tensor.shape[1]\n",
    "        height = data_tensor.shape[2]\n",
    "        width = data_tensor.shape[3]\n",
    "        print(f\"Image Dimensions: {height}x{width}\")\n",
    "        print(f\"Number of Channels: {num_channels}\")\n",
    "\n",
    "        # 3. Mean and Standard Deviation of pixel values per channel\n",
    "        # This is useful for data normalization\n",
    "        print(\"\\nPixel Value Statistics (per channel):\")\n",
    "        # Reshape to (N*H*W, C) to calculate stats per channel easily\n",
    "        pixels = data_tensor.permute(0, 2, 3, 1).reshape(-1, num_channels)\n",
    "        mean = pixels.mean(axis=0)\n",
    "        std = pixels.std(axis=0)\n",
    "\n",
    "        for c in range(num_channels):\n",
    "            print(f\"  Channel {c}: Mean={mean[c]:.4f}, Std Dev={std[c]:.4f}\")\n",
    "    \n",
    "    print(\"--------------------------------------\")\n",
    "\n",
    "\n",
    "def create_dummy_dataset(num_samples=1000, img_size=(64, 64), num_classes=10, channels=3):\n",
    "    \"\"\"\n",
    "    Creates a dummy dataset for demonstration purposes.\n",
    "    Returns:\n",
    "        (DataLoader, DataLoader): train_loader, test_loader\n",
    "    \"\"\"\n",
    "    print(\"Creating a dummy dataset...\")\n",
    "    # Generate random images and labels\n",
    "    data = np.random.rand(num_samples, channels, img_size[0], img_size[1]).astype(np.float32)\n",
    "    labels = np.random.randint(0, num_classes, num_samples).astype(np.int64)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    tensor_x = torch.Tensor(data)\n",
    "    tensor_y = torch.Tensor(labels).long()\n",
    "\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    \n",
    "    # Split into training and testing\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(\"Dummy dataset created.\")\n",
    "    return train_loader, test_loader\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e2c261-3b16-4510-ac2e-e6023d128ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhold_train, hold_test = create_dummy_dataset(num_samples=100, img_size=(1, 4096), num_classes=6, channels=2)\\n\\nprint(hold_train)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "hold_train, hold_test = create_dummy_dataset(num_samples=100, img_size=(1, 4096), num_classes=6, channels=2)\n",
    "\n",
    "print(hold_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f77177da-2d81-4666-bb73-d71df63d133a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(hold_train.dataset[0]))\\nprint((hold_train.dataset[0]))\\nprint(len(hold_train.dataset[0][0]))\\nprint((hold_train.dataset[0][0]))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(len(hold_train.dataset[0]))\n",
    "print((hold_train.dataset[0]))\n",
    "print(len(hold_train.dataset[0][0]))\n",
    "print((hold_train.dataset[0][0]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c17857-49d8-4154-a533-d18df80d1c6c",
   "metadata": {},
   "source": [
    "## Format RF data into data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e7834e5-d2d1-479d-939f-1b16385f8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def Format_RF_dataset(data, labels, batch_size = 32, train_test_split = 0.2):\n",
    "    \"\"\"\n",
    "    Formats labeled IQ data into dataloader\n",
    "    Returns:\n",
    "        (DataLoader, DataLoader): train_loader, test_loader\n",
    "    \"\"\"\n",
    "    print(\"Formatting RF dataset...\")\n",
    "    # Generate random images and labels\n",
    "    #data = np.random.rand(num_samples, channels, img_size[0], img_size[1]).astype(np.float32)\n",
    "    #labels = np.random.randint(0, num_classes, num_samples).astype(np.int64)\n",
    "\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    tensor_x = torch.Tensor(data)\n",
    "    tensor_y = torch.Tensor(labels).long()\n",
    "\n",
    "    dataset = TensorDataset(tensor_x, tensor_y)\n",
    "    \n",
    "    # Split into training and testing\n",
    "    train_size = int((1-train_test_split) * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(\"RF dataset loader created.\")\n",
    "    return train_loader, test_loader\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3abbde0d-51a9-43a4-8ccd-845a6a079ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_loader, test_loader= Format_RF_dataset(train_data, train_label, batch_size = 32, train_test_split = 0.2)\\nval_loader,_ = Format_RF_dataset(val_data, val_label, batch_size = 32, train_test_split = 0.2)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_loader, test_loader= Format_RF_dataset(train_data, train_label, batch_size = 32, train_test_split = 0.2)\n",
    "val_loader,_ = Format_RF_dataset(val_data, val_label, batch_size = 32, train_test_split = 0.2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b967a460-b3f5-46af-bcad-0b0bb18049fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(len(train_loader.dataset[0]))\\nprint((train_loader.dataset[0]))\\nprint(len(train_loader.dataset[0][0]))\\nprint((train_loader.dataset[0][0]))\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(len(train_loader.dataset[0]))\n",
    "print((train_loader.dataset[0]))\n",
    "print(len(train_loader.dataset[0][0]))\n",
    "print((train_loader.dataset[0][0]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c1fcd-38e9-4185-9906-1e2c378f7a8a",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc7f39-e7c7-41d2-be3b-2e78d248d967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num classes 9\n",
      "input length 1048576\n",
      "output channels 32\n",
      "[1. Dataset Creation & Analysis] starting...\n",
      "Formatting RF dataset...\n",
      "RF dataset loader created.\n",
      "Formatting RF dataset...\n",
      "RF dataset loader created.\n",
      "\n",
      "--- Analyzing Training Dataset ---\n",
      "Class Distribution:\n",
      "  Class 0: 8056 samples (11.19%)\n",
      "  Class 1: 8105 samples (11.26%)\n",
      "  Class 2: 7920 samples (11.00%)\n",
      "  Class 3: 7958 samples (11.05%)\n",
      "  Class 4: 8121 samples (11.28%)\n",
      "  Class 5: 7906 samples (10.98%)\n",
      "  Class 6: 7855 samples (10.91%)\n",
      "  Class 7: 7966 samples (11.06%)\n",
      "  Class 8: 8113 samples (11.27%)\n",
      "\n",
      "Total Samples: 72000\n",
      "Image Dimensions: 2x1048576\n",
      "Number of Channels: 1\n",
      "\n",
      "Pixel Value Statistics (per channel):\n",
      "  Channel 0: Mean=0.0002, Std Dev=353.4129\n",
      "--------------------------------------\n",
      "[1. Dataset Creation & Analysis] finished in 556.87 seconds.\n",
      "[2. Validation Split] starting...\n",
      "\n",
      "--- Creating Validation Split ---\n",
      "Split training data into 90000 training samples and 18000 validation samples.\n",
      "[2. Validation Split] finished in 0.00 seconds.\n",
      "[3. Model Creation] starting...\n",
      "\n",
      "--- Creating Model ---\n",
      "model intial in_channels:  1\n",
      "in features for first FC layer:  135168\n",
      "Using 8 GPUs for training!\n",
      "Model Architecture:\n",
      "DataParallel(\n",
      "  (module): DynamicCNN(\n",
      "    (conv_layers): ModuleList(\n",
      "      (0): Conv2d(1, 32, kernel_size=(2, 2), stride=(2, 2), padding=(1, 1))\n",
      "      (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "      (2): Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "      (3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "      (4): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "      (5): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "      (6): Conv2d(256, 512, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "      (7): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "      (8): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "      (9-10): 2 x Conv2d(1024, 1024, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "      (11-12): 2 x Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
      "      (13-14): 2 x Conv2d(1024, 1024, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (pool_layers): ModuleList(\n",
      "      (0-9): 10 x MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Identity()\n",
      "      (11-13): 3 x MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (14): Identity()\n",
      "    )\n",
      "    (fc_layers): ModuleList(\n",
      "      (0): Linear(in_features=135168, out_features=256, bias=True)\n",
      "      (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "    (output_layer): Linear(in_features=32, out_features=9, bias=True)\n",
      "  )\n",
      ")\n",
      "[3. Model Creation] finished in 9.63 seconds.\n",
      "\n",
      "--- Training Model ---\n",
      "Starting training...\n"
     ]
    }
   ],
   "source": [
    "#74% accuracy with output_channels = 16, batch size 32, lr = 0.0001 for PSKvAMvFM, 800 samples each, epoch = 30\n",
    "#77% w output_channels = 64 , batch_size = 8, epoch = 50\n",
    "#74% @ epochs = 100, 323.327 MB\n",
    "#67% w output_channels = 128, batch = 8, epoch = 30, 1293.217 MB\n",
    "#73% w output_channels = 256, batch = 8, epoch = 30, 5172.685 MB\n",
    "### All above with one FC layer = [output_channels*2*2*2*2, output_channels]\n",
    "#NOTE: FC layer should not scale 1:1 with the size of CNN layers, it quickly becomes too large\n",
    "#75.5% w output_channels = 256, batch_size = 32, epoch = 30, fc_config = [output_channels*2*2, output_channels], 3249.673 MB\n",
    "#output channel above 256 seems bad and slow\n",
    "#81.18 % test accuracy, 100% train accuracy, 500 epoch, same config as above\n",
    "\n",
    "num_classes = len(class_list)\n",
    "num_iq_samples\n",
    "output_channels = 32\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 0.0001 #learning rate\n",
    "print(f\"num classes {num_classes}\")\n",
    "print(f\"input length {num_iq_samples}\")\n",
    "print(f\"output channels {output_channels}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Configuration ---\n",
    "    config = {\n",
    "        'INPUT_CHANNELS': 1, #### going to try treating IQ as one input channel of 2d data input size *2\n",
    "        'NUM_CLASSES': num_classes,\n",
    "        'IMAGE_SIZE': (2,num_iq_samples),\n",
    "        'DEVICE': str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")),\n",
    "        'MODEL_SAVE_PATH': \"custom_cnn.pth\",\n",
    "        'REPORT_SAVE_PATH': \"training_report.txt\",\n",
    "        'NUM_EPOCHS': epochs,\n",
    "        'LEARNING_RATE': lr #good learning rates: 0.0001\n",
    "    }\n",
    "\n",
    "    # Define a custom CNN architecture\n",
    "    # Format: (out_channels, kernel_size, stride, padding, use_pooling)\n",
    "\n",
    "    '''\n",
    "    conv_config = [\n",
    "        (output_channels, 2, 1, 0, True),   # 1st Conv Layer -> MaxPool ##!!!removed padding ###!!!set kernel size to 1\n",
    "        (output_channels*2, 2, 1, 0, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2, 2, 1, 0, False)   # 3rd Conv Layer\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    ##!!!!This one works!!!!\n",
    "    conv_config = [\n",
    "        (16, 2, 1, 1, True),   # 1st Conv Layer -> MaxPool\n",
    "        (32, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (64, 2, 1, 1, False)   # 3rd Conv Layer\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    conv_config = [\n",
    "        (output_channels, 2, 2, 1, True),   # 1st Conv Layer -> MaxPool\n",
    "        (output_channels*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 2, 1, 1, False),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 1, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 2, 1, 1, True),   # 2nd Conv Layer -> MaxPool\n",
    "        (output_channels*2*2*2*2*2, 2, 1, 1, False),   # 2nd Conv Layer -> MaxPool\n",
    "        \n",
    "    ]\n",
    "\n",
    "    \n",
    "\n",
    "    # Format: list of output features for each fully connected layer\n",
    "    #fc_config = [output_channels*2*2*2*2, output_channels] ##!!!NOTE: this is good for smaller models\n",
    "    fc_config = [output_channels*2*2*2, output_channels*2, output_channels]\n",
    "    \n",
    "    # --- 1. Create a dummy dataset ---\n",
    "    # In a real scenario, you would replace this with your own data loading logic.\n",
    "    # Ensure your data is in the format of (N, C, H, W)\n",
    "    # N: Number of samples, C: Channels, H: Height, W: Width\n",
    "    \n",
    "    timings = {} # Dictionary to store timings of each step\n",
    "\n",
    "    with Timer(timings, \"1. Dataset Creation & Analysis\"):\n",
    "        train_loader, test_loader= Format_RF_dataset(train_data, train_label, batch_size = batch_size, train_test_split = 0.2)\n",
    "        val_loader,_ = Format_RF_dataset(val_data, val_label, batch_size = batch_size, train_test_split = 0.0)#train test split set to 0 for val set\n",
    "        # --- 1.5. Analyze the training data ---\n",
    "        analyze_dataset(train_loader, dataset_name=\"Training\")\n",
    "    \n",
    "    # --- 1.6. Create a validation split ---\n",
    "    with Timer(timings, \"2. Validation Split\"):\n",
    "        # In a real-world scenario, you would have a separate validation dataset.\n",
    "        # Here, we split the original training data for demonstration.\n",
    "        print(\"\\n--- Creating Validation Split ---\")\n",
    "        #train_dataset = train_loader.dataset\n",
    "        #val_split = 0.2\n",
    "        #dataset_size = len(train_dataset)\n",
    "        #val_size = int(val_split * dataset_size)\n",
    "        #train_size = dataset_size - val_size\n",
    "        \n",
    "        #new_train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "        \n",
    "        # Create new DataLoaders for the split data\n",
    "        #new_train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "        #val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "        print(f\"Split training data into {len(train_data)} training samples and {len(val_data)} validation samples.\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # --- 2. Create the CNN model ---\n",
    "    with Timer(timings, \"3. Model Creation\"):\n",
    "        print(\"\\n--- Creating Model ---\")\n",
    "        my_cnn = DynamicCNN(\n",
    "            input_channels=config['INPUT_CHANNELS'], \n",
    "            num_classes=config['NUM_CLASSES'],\n",
    "            conv_layers_config=conv_config,\n",
    "            fc_layers_config=fc_config,\n",
    "            num_iq_samples = config['IMAGE_SIZE']\n",
    "        )\n",
    "        \n",
    "        # Check for multiple GPUs and wrap the model with DataParallel for multi-GPU training\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for training!\")\n",
    "            my_cnn = nn.DataParallel(my_cnn)\n",
    "\n",
    "        model_summary_str = str(my_cnn)\n",
    "        print(\"Model Architecture:\")\n",
    "        print(model_summary_str)\n",
    "\n",
    "    # --- 3. Define Loss Function and Optimizer ---\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(my_cnn.parameters(), lr=config['LEARNING_RATE'])\n",
    "\n",
    "    # --- 4. Train the model ---\n",
    "    print(\"\\n--- Training Model ---\")\n",
    "    history, training_time = train_model(\n",
    "        my_cnn, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        num_epochs=config['NUM_EPOCHS'], \n",
    "        device=config['DEVICE']\n",
    "    )\n",
    "    timings[\"4. Model Training\"] = training_time\n",
    "\n",
    "\n",
    "    # --- 5. Plot training results ---\n",
    "    with Timer(timings, \"5. Plotting Results\"):\n",
    "        plot_results(history)\n",
    "\n",
    "    # --- 6. Test the model on the unseen test set ---\n",
    "    with Timer(timings, \"6. Final Model Testing\"):\n",
    "        print(\"\\n--- Testing Model ---\")\n",
    "        final_test_accuracy = test_model(my_cnn, test_loader, device=config['DEVICE'])\n",
    "\n",
    "    # --- 7. Save the model ---\n",
    "    with Timer(timings, \"7. Saving Model\"):\n",
    "        print(\"\\n--- Saving Model ---\")\n",
    "        save_model(my_cnn, path=config['MODEL_SAVE_PATH'])\n",
    "\n",
    "    # --- 8. Load the model ---\n",
    "    with Timer(timings, \"8. Loading Model\"):\n",
    "        print(\"\\n--- Loading Model ---\")\n",
    "        # We need to provide the same configuration to instantiate the model class\n",
    "        # before loading the saved weights.\n",
    "        loaded_model = load_model(\n",
    "            DynamicCNN,\n",
    "            path=config['MODEL_SAVE_PATH'],\n",
    "            input_channels=config['INPUT_CHANNELS'],\n",
    "            num_classes=config['NUM_CLASSES'],\n",
    "            conv_layers_config=conv_config,\n",
    "            fc_layers_config=fc_config,\n",
    "            num_iq_samples = config['IMAGE_SIZE']\n",
    "        )\n",
    "        print(\"Loaded Model Architecture:\")\n",
    "        print(loaded_model)\n",
    "\n",
    "    # --- 9. Test the loaded model to verify it's working ---\n",
    "    with Timer(timings, \"9. Testing Loaded Model\"):\n",
    "        print(\"\\n--- Testing Loaded Model ---\")\n",
    "        test_model(loaded_model, test_loader, device=config['DEVICE'])\n",
    "    \n",
    "    # --- 10. Report Timings ---\n",
    "    report_timings(timings)\n",
    "    # Calculate parameter size\n",
    "    param_size = sum(param.nelement() * param.element_size() for param in loaded_model.parameters())\n",
    "    \n",
    "    \n",
    "    # Calculate buffer size\n",
    "    buffer_size = sum(buffer.nelement() * buffer.element_size() for buffer in loaded_model.buffers())\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / 1024**2\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    model_summary_str = model_summary_str +\"\\n\"+(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    # --- 11. Save Final Report ---\n",
    "    with Timer(timings, \"10. Saving Report\"):\n",
    "        save_report(\n",
    "            report_path=config['REPORT_SAVE_PATH'],\n",
    "            config=config,\n",
    "            model_summary=model_summary_str,\n",
    "            history=history,\n",
    "            final_accuracy=final_test_accuracy,\n",
    "            timings=timings\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634b3b4f-7f9d-4719-865e-ab02e8159fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "'''\n",
    "1) make everything a script\n",
    "2) name the files dynamically with the experimental hyper-params\n",
    "3) be able to loop through training rounds\n",
    "4) save models with names relating to performance (only need to save models for extrememly long training times, for normal experiments can\n",
    "just keep the hyper-parameters and data generation specifics and re-create the experiment if needed.\n",
    "5) save all the training results and model names in a centralized location\n",
    "6) spectrograms!!!\n",
    "7) Embedding model\n",
    "8) Add residual layers from gemini code\n",
    "9) Add GUI???\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59572eb9-66a5-416e-a543-5414a4310a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
